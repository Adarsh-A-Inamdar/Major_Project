{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leukemia Type and Grade Classification Model\n",
    "\n",
    "This notebook builds and trains a multi-output Convolutional Neural Network (CNN) to classify leukemia images into four types (ALL, AML, CLL, CML) and three disease grades (Chronic, Accelerated, Blast Crisis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "First, we'll import the necessary libraries and define the main configuration parameters for our model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20 # Keep it low for a quick test run, increase for real training\n",
    "NUM_CLASSES_TYPE = 4\n",
    "NUM_CLASSES_GRADE = 3\n",
    "BASE_DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dummy Data Generation\n",
    "\n",
    "This section contains a helper function to create a fake dataset that mimics the expected directory structure (`data/ALL`, `data/AML`, etc.). This is useful for testing the entire pipeline without needing the actual dataset. It also generates a `labels.csv` file which is crucial for associating each image with its correct grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_data(base_dir, num_images_per_class=50):\n",
    "    \"\"\"\n",
    "    Generates a dummy image dataset and a corresponding labels CSV file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(base_dir):\n",
    "        print(f\"Directory '{base_dir}' already exists. Skipping data generation.\")\n",
    "        if os.path.exists('labels.csv'):\n",
    "            return pd.read_csv('labels.csv')\n",
    "        else:\n",
    "            # If directory exists but CSV is missing, we should still regenerate labels\n",
    "            print(\"labels.csv not found. Will regenerate labels.\")\n",
    "    \n",
    "    print(\"Generating dummy data...\")\n",
    "    leukemia_types = [\"ALL\", \"AML\", \"CLL\", \"CML\"]\n",
    "    grades = [\"Chronic\", \"Accelerated\", \"Blast_Crisis\"]\n",
    "    \n",
    "    records = []\n",
    "\n",
    "    for l_type in leukemia_types:\n",
    "        class_dir = os.path.join(base_dir, l_type)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for i in range(num_images_per_class):\n",
    "            # Create a random noise image\n",
    "            dummy_image_array = np.random.rand(IMG_SIZE[0], IMG_SIZE[1], 3) * 255\n",
    "            \n",
    "            # Assign a random grade\n",
    "            grade = np.random.choice(grades)\n",
    "            \n",
    "            # Define file path\n",
    "            img_filename = f\"{l_type}_{i}_{grade}.png\"\n",
    "            img_filepath = os.path.join(class_dir, img_filename)\n",
    "\n",
    "            # Save the image\n",
    "            save_img(img_filepath, dummy_image_array)\n",
    "\n",
    "            # Store record for CSV\n",
    "            records.append({\n",
    "                \"filepath\": img_filepath,\n",
    "                \"leukemia_type\": l_type,\n",
    "                \"grade\": grade\n",
    "            })\n",
    "\n",
    "    # Create and save the labels DataFrame\n",
    "    labels_df = pd.DataFrame(records)\n",
    "    labels_df.to_csv(\"labels.csv\", index=False)\n",
    "    print(\"Dummy data generation complete.\")\n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "Here, we define functions to create an efficient `tf.data.Dataset`. This involves:\n",
    "1.  Reading the image files.\n",
    "2.  Decoding, resizing, and normalizing the images.\n",
    "3.  Mapping string labels (like 'ALL', 'Chronic') to one-hot encoded vectors.\n",
    "4.  Shuffling, batching, and prefetching the data for optimal performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(filepath, label_type, label_grade):\n",
    "    \"\"\"\n",
    "    Loads an image from filepath, decodes it, resizes, and normalizes it.\n",
    "    \"\"\"\n",
    "    img = tf.io.read_file(filepath)\n",
    "    # Ensure we decode as PNG, even if file extension varies. Handles potential format issues.\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, [IMG_SIZE[0], IMG_SIZE[1]])\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "    return img, (label_type, label_grade)\n",
    "\n",
    "def create_dataset(df, type_map, grade_map):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow Dataset from a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    filepaths = df['filepath'].values\n",
    "    \n",
    "    # Map string labels to integer indices\n",
    "    type_labels = df['leukemia_type'].map(type_map).values\n",
    "    grade_labels = df['grade'].map(grade_map).values\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    type_labels_one_hot = to_categorical(type_labels, num_classes=NUM_CLASSES_TYPE)\n",
    "    grade_labels_one_hot = to_categorical(grade_labels, num_classes=NUM_CLASSES_GRADE)\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, type_labels_one_hot, grade_labels_one_hot))\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=len(df)).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture (Multi-Output CNN)\n",
    "\n",
    "This is the core of the project. We build a multi-output model:\n",
    "- **Shared Base:** A stack of `Conv2D` and `MaxPooling2D` layers that learn to extract common visual features from the images.\n",
    "- **Type Branch:** A dedicated dense head that classifies the leukemia type.\n",
    "- **Grade Branch:** A second dense head that classifies the disease grade based on features like cell density, which are learned by the shared base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_output_model(input_shape, num_types, num_grades):\n",
    "    \"\"\"\n",
    "    Builds a multi-output CNN model.\n",
    "    \"\"\"\n",
    "    # Input Layer\n",
    "    inputs = Input(shape=input_shape, name=\"input_layer\")\n",
    "\n",
    "    # --- Shared Convolutional Base ---\n",
    "    # This part of the network learns common features from the images.\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # --- Branch 1: Leukemia Type Classification ---\n",
    "    type_branch = Dense(256, activation='relu')(x)\n",
    "    type_branch = Dropout(0.5)(type_branch)\n",
    "    type_output = Dense(num_types, activation='softmax', name='type_output')(type_branch)\n",
    "\n",
    "    # --- Branch 2: Disease Grade Classification ---\n",
    "    # The grade is determined by cell density, a feature the CNN will learn to recognize.\n",
    "    grade_branch = Dense(256, activation='relu')(x)\n",
    "    grade_branch = Dropout(0.5)(grade_branch)\n",
    "    grade_output = Dense(num_grades, activation='softmax', name='grade_output')(grade_branch)\n",
    "\n",
    "    # --- Create and Compile the Model ---\n",
    "    model = Model(inputs=inputs, outputs=[type_output, grade_output], name=\"leukemia_classifier\")\n",
    "\n",
    "    # We define separate losses for each output branch\n",
    "    losses = {\n",
    "        'type_output': 'categorical_crossentropy',\n",
    "        'grade_output': 'categorical_crossentropy'\n",
    "    }\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=losses,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Function\n",
    "\n",
    "A helper function to plot the training and validation accuracy and loss for both output branches. This helps in diagnosing issues like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plots training and validation loss and accuracy.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Model Training History', fontsize=16)\n",
    "\n",
    "    # Plot Type Accuracy\n",
    "    axes[0, 0].plot(history.history['type_output_accuracy'], label='Train Type Acc')\n",
    "    axes[0, 0].plot(history.history['val_type_output_accuracy'], label='Val Type Acc')\n",
    "    axes[0, 0].set_title('Leukemia Type Accuracy')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].legend(loc='lower right')\n",
    "\n",
    "    # Plot Grade Accuracy\n",
    "    axes[0, 1].plot(history.history['grade_output_accuracy'], label='Train Grade Acc')\n",
    "    axes[0, 1].plot(history.history['val_grade_output_accuracy'], label='Val Grade Acc')\n",
    "    axes[0, 1].set_title('Disease Grade Accuracy')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].legend(loc='lower right')\n",
    "\n",
    "    # Plot Type Loss\n",
    "    axes[1, 0].plot(history.history['type_output_loss'], label='Train Type Loss')\n",
    "    axes[1, 0].plot(history.history['val_type_output_loss'], label='Val Type Loss')\n",
    "    axes[1, 0].set_title('Leukemia Type Loss')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].legend(loc='upper right')\n",
    "\n",
    "    # Plot Grade Loss\n",
    "    axes[1, 1].plot(history.history['grade_output_loss'], label='Train Grade Loss')\n",
    "    axes[1, 1].plot(history.history['val_grade_output_loss'], label='Val Grade Loss')\n",
    "    axes[1, 1].set_title('Disease Grade Loss')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(\"training_history.png\")\n",
    "    print(\"\\nSaved training history plot to 'training_history.png'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Execution\n",
    "\n",
    "This is where we bring everything together. We'll execute the steps in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Generate or Load Data\n",
    "\n",
    "We call the function to generate dummy data. If you are using your own data, you should **comment out this cell** and instead load your own CSV file with `pd.read_csv('your_labels.csv')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = generate_dummy_data(BASE_DATA_DIR)\n",
    "print(f\"Loaded {len(labels_df)} image records.\")\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Create Label Mappings and Split Data\n",
    "\n",
    "The model needs numerical labels, so we create dictionaries to map our string labels (e.g., 'ALL') to integers (e.g., 0). We then split our data into training, validation, and testing sets to ensure we can evaluate the model on data it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label-to-integer mappings\n",
    "type_classes = sorted(labels_df['leukemia_type'].unique())\n",
    "grade_classes = sorted(labels_df['grade'].unique())\n",
    "\n",
    "type_map = {label: i for i, label in enumerate(type_classes)}\n",
    "grade_map = {label: i for i, label in enumerate(grade_classes)}\n",
    "\n",
    "# Create integer-to-label reverse mappings for prediction\n",
    "inv_type_map = {i: label for label, i in type_map.items()}\n",
    "inv_grade_map = {i: label for label, i in grade_map.items())\n",
    "\n",
    "print(\"Type Map:\", type_map)\n",
    "print(\"Grade Map:\", grade_map)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "# Stratify ensures that the class distribution is similar across all splits\n",
    "train_df, test_df = train_test_split(labels_df, test_size=0.2, random_state=42, stratify=labels_df['leukemia_type'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['leukemia_type'])\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"Training samples:   {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples:       {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Create TensorFlow Datasets\n",
    "\n",
    "We now convert our pandas DataFrames into `tf.data.Dataset` objects. This is the standard and most efficient way to feed data into a TensorFlow model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_df, type_map, grade_map)\n",
    "val_dataset = create_dataset(val_df, type_map, grade_map)\n",
    "test_dataset = create_dataset(test_df, type_map, grade_map)\n",
    "\n",
    "print(\"Datasets created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Build and Compile the Model\n",
    "\n",
    "We instantiate the model architecture we defined earlier and print a summary to visualize its layers and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_multi_output_model(\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n",
    "    num_types=NUM_CLASSES_TYPE,\n",
    "    num_grades=NUM_CLASSES_GRADE\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Optionally, save a plot of the model architecture\n",
    "plot_model(model, to_file='model_architecture.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Train the Model\n",
    "\n",
    "Now, we fit the model to our training data. We also provide the validation data so we can monitor the model's performance on a separate data split after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Model Training ---\")\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "print(\"--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Evaluate the Model\n",
    "\n",
    "After training, we evaluate the final model's performance on the test set. This gives us a final, unbiased measure of how well our model is likely to perform on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating Model on Test Data ---\")\n",
    "results = model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Test Loss (Total): {results[0]:.4f}\")\n",
    "print(f\"Test Loss (Type):  {results[1]:.4f}\")\n",
    "print(f\"Test Loss (Grade): {results[2]:.4f}\")\n",
    "print(f\"Test Accuracy (Type): {results[3]:.4f}\")\n",
    "print(f\"Test Accuracy (Grade): {results[4]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7. Visualize Training History\n",
    "\n",
    "Plotting the training history helps us understand how the model learned over time. We look for the convergence of training and validation curves, and signs of overfitting (where validation performance gets worse while training performance improves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8. Run an Example Prediction\n",
    "\n",
    "Finally, let's take a single image from our test set and see what the model predicts for both the leukemia type and disease grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Example Prediction ---\")\n",
    "# Take one batch from the test set for demonstration\n",
    "for images, (labels_type, labels_grade) in test_dataset.take(1):\n",
    "    sample_image = images[0:1] # Get the first image, keep batch dimension\n",
    "    true_label_type = inv_type_map[np.argmax(labels_type[0])]\n",
    "    true_label_grade = inv_grade_map[np.argmax(labels_grade[0])]\n",
    "\n",
    "    predictions = model.predict(sample_image)\n",
    "    pred_type_idx = np.argmax(predictions[0])\n",
    "    pred_grade_idx = np.argmax(predictions[1])\n",
    "\n",
    "    predicted_type = inv_type_map[pred_type_idx]\n",
    "    predicted_grade = inv_grade_map[pred_grade_idx]\n",
    "\n",
    "    print(f\"\\nSample Image Analysis:\")\n",
    "    print(f\"  > True Leukemia Type: {true_label_type}\")\n",
    "    print(f\"  > Predicted Leukemia Type: {predicted_type}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"  > True Disease Grade: {true_label_grade}\")\n",
    "    print(f\"  > Predicted Disease Grade: {predicted_grade}\")\n",
    "    \n",
    "    # Display the image being predicted\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(sample_image[0])\n",
    "    plt.title(f\"Predicted Type: {predicted_type}\\nPredicted Grade: {predicted_grade.replace('_', ' ')}\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"prediction_example.png\")\n",
    "    print(\"\\nSaved example prediction image to 'prediction_example.png'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean Up (Optional)\n",
    "\n",
    "Run this cell to remove the dummy data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree(BASE_DATA_DIR)\n",
    "# print(f\"Cleaned up dummy data directory: '{BASE_DATA_DIR}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
